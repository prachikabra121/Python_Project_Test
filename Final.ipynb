{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970eed16-6367-4ecb-8adf-585c9be9d378",
   "metadata": {},
   "source": [
    "<h3>Example to read the data from .txt file with legth of each column and start index of each column</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdfcc-6ac7-4400-aae7-ff487efc43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '1234567890_20241110_CUST00123456789.txt'  # Replace with the actual file path\n",
    "\n",
    "# Column specifications based on the sample data's fixed-width structure\n",
    "colspecs = [(0, 10), (11, 18), (20, 36), (37, 53), (54, 56), (57, 63)]\n",
    "columns = [\"Transaction_ID\", \"Date\", \"Customer_ID\", \"Product_ID\", \"Quantity\", \"Price\"]\n",
    "\n",
    "# Read the fixed-width data from the raw file\n",
    "df = pd.read_fwf(file_path, colspecs=colspecs, names=columns)\n",
    "\n",
    "# Clean the data if needed (strip any leading/trailing spaces)\n",
    "df['Transaction_ID'] = df['Transaction_ID'].str.replace('\"', '').str.strip()\n",
    "df['Customer_ID'] = df['Customer_ID'].str.strip()\n",
    "df['Product_ID'] = df['Product_ID'].str.strip()\n",
    "\n",
    "# Convert Quantity and Price columns to numeric, force errors to NaN\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de83d9b-93dd-446d-bbaf-5b787bf7c3e4",
   "metadata": {},
   "source": [
    "<h3>Extracting the starting index of our columns and value of respective columns to read our example string</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "513af958-d157-4600-b942-6545571ef6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: RPOSKBN, Starting Index: 1, Value: 0\n",
      "Column: RMAINID, Starting Index: 2, Value: 235\n",
      "Column: RSUBID, Starting Index: 5, Value: 00\n",
      "Column: RTENCD, Starting Index: 7, Value: 0003\n",
      "Column: RTC, Starting Index: 11, Value: 00\n",
      "Column: RYMD, Starting Index: 13, Value: 240201\n",
      "Column: RTIME, Starting Index: 19, Value: 1700\n",
      "Column: HDHMS, Starting Index: 23, Value: 01170003\n",
      "Column: SYUKBN, Starting Index: 31, Value: 0\n",
      "Column: FILLER, Starting Index: 32, Value: 00\n",
      "Column: StoreNo, Starting Index: 34, Value: 0030\n",
      "Column: RecordType, Starting Index: 38, Value: 10\n",
      "Column: TransType, Starting Index: 40, Value: 00\n",
      "Column: FroorNo, Starting Index: 42, Value: 010\n",
      "Column: RegNo, Starting Index: 45, Value: 072\n",
      "Column: Day, Starting Index: 48, Value: 02402011\n",
      "Column: Time, Starting Index: 56, Value: 600061\n",
      "Column: ReceiptNo, Starting Index: 62, Value: 6211\n",
      "Column: SeisanType, Starting Index: 66, Value: 0\n",
      "Column: CheckerNo, Starting Index: 67, Value: 000000705\n",
      "Column: CashierNo, Starting Index: 76, Value: 000000000\n",
      "Column: SequenceNo, Starting Index: 85, Value: 0000\n",
      "Column: DcTransNo, Starting Index: 89, Value: 6908\n",
      "Column: AccountCode, Starting Index: 93, Value: 0\n",
      "Column: KyaKuSu, Starting Index: 94, Value: 000001\n",
      "Column: UriageSu, Starting Index: 100, Value: 000006\n",
      "Column: UriageKingaku, Starting Index: 106, Value: 00001990\n",
      "Column: MotoTankaSu, Starting Index: 114, Value: 000006\n",
      "Column: MotoTankaKingaku, Starting Index: 120, Value: 00001990\n",
      "Column: UriageGenka, Starting Index: 128, Value: 00001366\n",
      "Column: ArariGaku, Starting Index: 136, Value: 00000530\n",
      "Column: NesageSu1, Starting Index: 144, Value: 000000\n",
      "Column: NesageKingaku1, Starting Index: 150, Value: 00000000\n",
      "Column: NesageSu2, Starting Index: 158, Value: 000000\n",
      "Column: NesageKingaku2, Starting Index: 164, Value: 00000000\n",
      "Column: NesageSu3, Starting Index: 172, Value: 000001\n",
      "Column: NesageKingaku3, Starting Index: 178, Value: 00000091\n",
      "Column: NesageSu4, Starting Index: 186, Value: 000000\n",
      "Column: NesageKingaku4, Starting Index: 192, Value: 00000000\n",
      "Column: NesageSu5, Starting Index: 200, Value: 000000\n",
      "Column: NesageKingaku5, Starting Index: 206, Value: 00000000\n",
      "Column: NesageSu6, Starting Index: 214, Value: 000000\n",
      "Column: NesageKingaku6, Starting Index: 220, Value: 00000000\n",
      "Column: NesageSu7, Starting Index: 228, Value: 000000\n",
      "Column: NesageKingaku7, Starting Index: 234, Value: 00000000\n",
      "Column: KyakuwariSu, Starting Index: 242, Value: 000000\n",
      "Column: KyakuwariKingaku, Starting Index: 248, Value: 00000000\n",
      "Column: DeputCode, Starting Index: 256, Value: 000000\n",
      "Column: ClassCode, Starting Index: 262, Value: 000000\n",
      "Column: PLUCode, Starting Index: 268, Value: 0000000000000\n",
      "Column: PLU2ndCode, Starting Index: 281, Value: 0000000000000\n",
      "Column: SKUKubun, Starting Index: 294, Value: 0\n",
      "Column: SKUCode, Starting Index: 295, Value: 0000000000000\n",
      "Column: KikakuKubun, Starting Index: 308, Value: 0\n",
      "Column: KikakuCode, Starting Index: 309, Value: 000000000\n",
      "Column: SeikakuKubun, Starting Index: 318, Value: 0\n",
      "Column: BMNo, Starting Index: 319, Value: 000000\n",
      "Column: TaxKubun, Starting Index: 325, Value: 00\n",
      "Column: SotoZeiKingaku, Starting Index: 327, Value: 00000170\n",
      "Column: UtiZeiKingaku, Starting Index: 335, Value: 00000000\n",
      "Column: NyukinCode, Starting Index: 343, Value: 000000\n",
      "Column: NyukinKingaku, Starting Index: 349, Value: 00000000\n",
      "Column: SyukinCode, Starting Index: 357, Value: 000000\n",
      "Column: SyukinKingaku, Starting Index: 363, Value: 00000000\n",
      "Column: Credit, Starting Index: 371, Value: 00000000\n",
      "Column: CreditSignLess, Starting Index: 379, Value: 00000000\n",
      "Column: CreditManual, Starting Index: 387, Value: 00000000\n",
      "Column: CreditCancel, Starting Index: 395, Value: 00000000\n",
      "Column: CreditCancelManual, Starting Index: 403, Value: 00000000\n",
      "Column: PointKangengaku, Starting Index: 411, Value: 00000000\n",
      "Column: GengaiCode, Starting Index: 419, Value: 000\n",
      "Column: GengaiKingaku, Starting Index: 422, Value: 00000000\n",
      "Column: TurisenYojyoukin, Starting Index: 430, Value: 00000000\n",
      "Column: Genkin, Starting Index: 438, Value: 00002069\n",
      "Column: MenberCode, Starting Index: 446, Value: 2126800371480100\n",
      "Column: MenberRank, Starting Index: 462, Value: 0\n",
      "Column: KoinkaiPoint, Starting Index: 463, Value: 54000\n",
      "Column: TujyoPoint, Starting Index: 468, Value: 54000\n",
      "Column: TanpinPoint, Starting Index: 473, Value: 00000\n",
      "Column: BunruiPoint, Starting Index: 478, Value: 00000\n",
      "Column: SyoukeiPoint, Starting Index: 483, Value: 00000\n",
      "Column: EcologyPoint, Starting Index: 488, Value: 00000\n",
      "Column: BirthDayFlag, Starting Index: 493, Value: 0\n",
      "Column: BonusFlag, Starting Index: 494, Value: 0\n",
      "Column: KangenPoint, Starting Index: 495, Value: 00000\n",
      "Column: TotalPoint, Starting Index: 500, Value: 00000\n",
      "Column: Toukadoubi, Starting Index: 505, Value: 00004702\n",
      "Column: Filler, Starting Index: 513, Value: 02402010\n"
     ]
    }
   ],
   "source": [
    "# Column names and lengths as provided\n",
    "columns = [\n",
    "    \"RPOSKBN\", \"RMAINID\", \"RSUBID\", \"RTENCD\", \"RTC\", \"RYMD\", \"RTIME\",\n",
    "    \"HDHMS\", \"SYUKBN\", \"FILLER\", \"StoreNo\", \"RecordType\", \"TransType\", \"FroorNo\", \"RegNo\", \n",
    "    \"Day\", \"Time\", \"ReceiptNo\", \"SeisanType\", \"CheckerNo\", \"CashierNo\", \"SequenceNo\", \"DcTransNo\", \n",
    "    \"AccountCode\", \"KyaKuSu\", \"UriageSu\", \"UriageKingaku\", \"MotoTankaSu\", \"MotoTankaKingaku\", \"UriageGenka\", \n",
    "    \"ArariGaku\", \"NesageSu1\", \"NesageKingaku1\", \"NesageSu2\", \"NesageKingaku2\", \"NesageSu3\", \"NesageKingaku3\", \n",
    "    \"NesageSu4\", \"NesageKingaku4\", \"NesageSu5\", \"NesageKingaku5\", \"NesageSu6\", \"NesageKingaku6\", \"NesageSu7\", \n",
    "    \"NesageKingaku7\", \"KyakuwariSu\", \"KyakuwariKingaku\", \"DeputCode\", \"ClassCode\", \"PLUCode\", \"PLU2ndCode\", \n",
    "    \"SKUKubun\", \"SKUCode\", \"KikakuKubun\", \"KikakuCode\", \"SeikakuKubun\", \"BMNo\", \"TaxKubun\", \"SotoZeiKingaku\", \n",
    "    \"UtiZeiKingaku\", \"NyukinCode\", \"NyukinKingaku\", \"SyukinCode\", \"SyukinKingaku\", \"Credit\", \"CreditSignLess\", \n",
    "    \"CreditManual\", \"CreditCancel\", \"CreditCancelManual\", \"PointKangengaku\", \"GengaiCode\", \"GengaiKingaku\", \n",
    "    \"TurisenYojyoukin\", \"Genkin\", \"MenberCode\", \"MenberRank\", \"KoinkaiPoint\", \"TujyoPoint\", \"TanpinPoint\", \n",
    "    \"BunruiPoint\", \"SyoukeiPoint\", \"EcologyPoint\", \"BirthDayFlag\", \"BonusFlag\", \"KangenPoint\", \"TotalPoint\", \n",
    "    \"Toukadoubi\", \"Filler\"\n",
    "]\n",
    "\n",
    "column_lengths = [\n",
    "    1, 3, 2, 4, 2, 6, 4, 8, 1, 2, 4, 2, 2, 3, 3, 8, 6, 4, 1, 9, 9, 4, 4, 1, 6, 6, 8, 6, 8, 8, 8, 6, \n",
    "    8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 6, 13, 13, 1, 13, 1, 9, 1, 6, 2, 8, 8, 6, 8, 6, \n",
    "    8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 16, 1, 5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 8, 8\n",
    "]\n",
    "\n",
    "# The input string\n",
    "input_string = \"0235000003002402011700011700030000030100001007202402011600061621100000007050000000000000690800000010000060000199000000600001990000013660000053000000000000000000000000000000000010000009100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000170000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002069212680037148010005400054000000000000000000000000000000000000000470202402010\"\n",
    "\n",
    "# Function to calculate the starting index for each column\n",
    "def calculate_starting_indexes(column_lengths):\n",
    "    start_indexes = []\n",
    "    current_start = 1  # The first column starts at index 1\n",
    "    for length in column_lengths:\n",
    "        start_indexes.append(current_start)\n",
    "        current_start += length  # Move the start to the next column's beginning\n",
    "    return start_indexes\n",
    "\n",
    "# Calculate the start indexes for each column\n",
    "start_indexes = calculate_starting_indexes(column_lengths)\n",
    "\n",
    "# Function to extract values based on the starting index and column length\n",
    "def extract_values(input_string, start_indexes, column_lengths):\n",
    "    extracted_data = []\n",
    "    for start, length in zip(start_indexes, column_lengths):\n",
    "        # Extract the substring based on start index and length\n",
    "        end = start + length - 1  # Calculate the ending index (inclusive)\n",
    "        value = input_string[start-1:end]  # Substring extraction (adjusting for 0-based index)\n",
    "        extracted_data.append(value)\n",
    "    return extracted_data\n",
    "\n",
    "# Extract the values\n",
    "extracted_values = extract_values(input_string, start_indexes, column_lengths)\n",
    "\n",
    "# Now, print the column name, starting index, and the corresponding value\n",
    "for column, start_index, value in zip(columns, start_indexes, extracted_values):\n",
    "    print(f\"Column: {column}, Starting Index: {start_index}, Value: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d90263-0bd9-4667-85eb-2eff086ae9ae",
   "metadata": {},
   "source": [
    "<h3>Extracted the schema into excel</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f3d047f-4024-4b66-bebf-008cc4e1aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to extracted_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Column names and lengths as provided\n",
    "columns = [\n",
    "    \"RPOSKBN\", \"RMAINID\", \"RSUBID\", \"RTENCD\", \"RTC\", \"RYMD\", \"RTIME\",\n",
    "    \"HDHMS\", \"SYUKBN\", \"FILLER\", \"StoreNo\", \"RecordType\", \"TransType\", \"FroorNo\", \"RegNo\", \n",
    "    \"Day\", \"Time\", \"ReceiptNo\", \"SeisanType\", \"CheckerNo\", \"CashierNo\", \"SequenceNo\", \"DcTransNo\", \n",
    "    \"AccountCode\", \"KyaKuSu\", \"UriageSu\", \"UriageKingaku\", \"MotoTankaSu\", \"MotoTankaKingaku\", \"UriageGenka\", \n",
    "    \"ArariGaku\", \"NesageSu1\", \"NesageKingaku1\", \"NesageSu2\", \"NesageKingaku2\", \"NesageSu3\", \"NesageKingaku3\", \n",
    "    \"NesageSu4\", \"NesageKingaku4\", \"NesageSu5\", \"NesageKingaku5\", \"NesageSu6\", \"NesageKingaku6\", \"NesageSu7\", \n",
    "    \"NesageKingaku7\", \"KyakuwariSu\", \"KyakuwariKingaku\", \"DeputCode\", \"ClassCode\", \"PLUCode\", \"PLU2ndCode\", \n",
    "    \"SKUKubun\", \"SKUCode\", \"KikakuKubun\", \"KikakuCode\", \"SeikakuKubun\", \"BMNo\", \"TaxKubun\", \"SotoZeiKingaku\", \n",
    "    \"UtiZeiKingaku\", \"NyukinCode\", \"NyukinKingaku\", \"SyukinCode\", \"SyukinKingaku\", \"Credit\", \"CreditSignLess\", \n",
    "    \"CreditManual\", \"CreditCancel\", \"CreditCancelManual\", \"PointKangengaku\", \"GengaiCode\", \"GengaiKingaku\", \n",
    "    \"TurisenYojyoukin\", \"Genkin\", \"MenberCode\", \"MenberRank\", \"KoinkaiPoint\", \"TujyoPoint\", \"TanpinPoint\", \n",
    "    \"BunruiPoint\", \"SyoukeiPoint\", \"EcologyPoint\", \"BirthDayFlag\", \"BonusFlag\", \"KangenPoint\", \"TotalPoint\", \n",
    "    \"Toukadoubi\", \"Filler\"\n",
    "]\n",
    "\n",
    "column_lengths = [\n",
    "    1, 3, 2, 4, 2, 6, 4, 8, 1, 2, 4, 2, 2, 3, 3, 8, 6, 4, 1, 9, 9, 4, 4, 1, 6, 6, 8, 6, 8, 8, 8, 6, \n",
    "    8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 6, 13, 13, 1, 13, 1, 9, 1, 6, 2, 8, 8, 6, 8, 6, \n",
    "    8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 16, 1, 5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 8, 8\n",
    "]\n",
    "\n",
    "# The input string\n",
    "input_string = \"0235000003002402011700011700030000030100001007202402011600061621100000007050000000000000690800000010000060000199000000600001990000013660000053000000000000000000000000000000000010000009100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000170000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002069212680037148010005400054000000000000000000000000000000000000000470202402010\"\n",
    "\n",
    "# Function to calculate the starting index for each column\n",
    "def calculate_starting_indexes(column_lengths):\n",
    "    start_indexes = []\n",
    "    current_start = 1  # The first column starts at index 1\n",
    "    for length in column_lengths:\n",
    "        start_indexes.append(current_start)\n",
    "        current_start += length  # Move the start to the next column's beginning\n",
    "    return start_indexes\n",
    "\n",
    "# Calculate the start indexes for each column\n",
    "start_indexes = calculate_starting_indexes(column_lengths)\n",
    "\n",
    "# Function to extract values based on the starting index and column length\n",
    "def extract_values(input_string, start_indexes, column_lengths):\n",
    "    extracted_data = []\n",
    "    for start, length in zip(start_indexes, column_lengths):\n",
    "        # Extract the substring based on start index and length\n",
    "        end = start + length - 1  # Calculate the ending index (inclusive)\n",
    "        value = input_string[start-1:end]  # Substring extraction (adjusting for 0-based index)\n",
    "        extracted_data.append(value)\n",
    "    return extracted_data\n",
    "\n",
    "# Extract the values\n",
    "extracted_values = extract_values(input_string, start_indexes, column_lengths)\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = {\n",
    "    \"Column\": columns,\n",
    "    \"Starting Index\": start_indexes,\n",
    "    \"Extracted Value\": extracted_values\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = 'extracted_data.xlsx'\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c04c7-2aa0-4ce0-89e8-d25d2ec1605d",
   "metadata": {},
   "source": [
    "<h3>extracted data from gz file and stored in excel file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc53ff3a-fa9c-4a39-8960-1e0ea379c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully extracted and saved to extracted_data_from_gz_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# Read the schema (Excel file with column names, start indexes, and column lengths)\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "\n",
    "# Extract column names, starting indexes, and lengths from the schema DataFrame\n",
    "columns = df_schema['Column'].tolist()\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Starting index for each column\n",
    "column_lengths = df_schema['Len'].tolist()  # Length of each column in data\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520  # Set as per your example\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        # Read the entire gzipped file into memory\n",
    "        file_content = f.read()\n",
    "    \n",
    "    # Calculate the number of rows (divide the total file length by 520 bytes)\n",
    "    total_rows = len(file_content) // chunk_size\n",
    "    \n",
    "    # List to hold all the extracted rows\n",
    "    all_rows = []\n",
    "\n",
    "    # Process the file in chunks of 520 bytes\n",
    "    for row_index in range(total_rows):\n",
    "        start = row_index * chunk_size\n",
    "        end = start + chunk_size\n",
    "        row_data = file_content[start:end]\n",
    "        \n",
    "        # Extract the data based on the schema (start indexes and column lengths)\n",
    "        row_values = []\n",
    "        for i, start in enumerate(start_indexes):\n",
    "            length = column_lengths[i]\n",
    "            if length is not None:\n",
    "                # Adjust for 1-based to 0-based index:\n",
    "                # The start in the schema is 1-based, so we subtract 1 for 0-based indexing.\n",
    "                start_idx = start - 1  # Adjust the start index (1-based to 0-based)\n",
    "                end_idx = start_idx + length  # The length determines how much data to extract\n",
    "                \n",
    "                value = row_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()  # Extract the value\n",
    "                row_values.append(value)\n",
    "        \n",
    "        # Append the extracted row to the result list\n",
    "        all_rows.append(row_values)\n",
    "    \n",
    "    # Create DataFrame from extracted data\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# File path to your gzipped file\n",
    "gz_file_path = 'example.txt.gz'  # Path to your gzipped file\n",
    "\n",
    "# Process the file\n",
    "df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "\n",
    "# Save the result to Excel\n",
    "output_file = 'extracted_data_from_gz_file.xlsx'  # Output file name\n",
    "df_extracted.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eafb66-1834-412e-b621-a4015ee9552d",
   "metadata": {},
   "source": [
    "<h3>Reading data by chunks one by one to optimize our code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fb22e6d-6734-4048-bc01-23b65e1f19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully extracted and saved to extracted_data_from_R520000220240201120001_2024020112050021076.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import itertools\n",
    "\n",
    "# Read the schema (Excel file with column names, start indexes, and column lengths)\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "\n",
    "# Extract column names, starting indexes, and lengths from the schema DataFrame\n",
    "columns = df_schema['Column'].tolist()\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Starting index for each column\n",
    "column_lengths = df_schema['Len'].tolist()  # Length of each column in data\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520  # Set as per your example\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "\n",
    "    # Open the gzipped file for reading in binary mode\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        # Read the file in chunks of 'chunk_size'\n",
    "        while True:\n",
    "            # Read the next chunk of data (520 bytes)\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            \n",
    "            # If the chunk is empty, we've reached the end of the file\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            \n",
    "            # Extract data based on the schema (start indexes and column lengths)\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    # Adjust for 1-based to 0-based index:\n",
    "                    # The start in the schema is 1-based, so we subtract 1 for 0-based indexing.\n",
    "                    start_idx = start - 1  # Adjust the start index (1-based to 0-based)\n",
    "                    end_idx = start_idx + length  # The length determines how much data to extract\n",
    "                    \n",
    "                    # Extract the value from the chunk\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()  # Extract the value\n",
    "                    row_values.append(value)\n",
    "\n",
    "            # Append the extracted row to the result list\n",
    "            all_rows.append(row_values)\n",
    "\n",
    "    # Create a DataFrame from the collected rows\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# File path to your gzipped file\n",
    "gz_file_path = 'R520000220240201120001_2024020112050021076.gz'  # Path to your gzipped file\n",
    "\n",
    "# Process the file\n",
    "df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "\n",
    "# Save the result to Excel\n",
    "output_file = 'extracted_data_from_R520000220240201120001_2024020112050021076.xlsx'  # Output file name\n",
    "df_extracted.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620a7fc-e3ef-4b9b-8a7b-a3e9c7b26d49",
   "metadata": {},
   "source": [
    "<h3>To convert our normal raw data into gz format</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fe91464a-93f5-4f1f-b9a6-43ef7fdf3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully compressed into: R520000320240201170000_2024020117050024632.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def convert_raw_to_gz(input_file_path, output_gz_path):\n",
    "    # Open the raw file in binary read mode\n",
    "    with open(input_file_path, 'rb') as f_in:\n",
    "        # Open the output .gz file in binary write mode\n",
    "        with gzip.open(output_gz_path, 'wb') as f_out:\n",
    "            # Use shutil.copyfileobj to copy the raw file content to the gz file\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    print(f\"File successfully compressed into: {output_gz_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'R520000320240201170000_2024020117050024632'  # Raw file path\n",
    "output_gz_file = 'R520000320240201170000_2024020117050024632.gz'  # Output .gz file path\n",
    "\n",
    "# Convert raw file to .gz\n",
    "convert_raw_to_gz(input_file, output_gz_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1205740-b282-412c-ae2e-da86c2fa5029",
   "metadata": {},
   "source": [
    "<h3>To load the data from multiple files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dfd8640b-7aa7-455d-af7c-e0d2eace1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully extracted and saved to extracted_data_combined.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Function to sanitize sheet names (remove illegal characters for Excel)\n",
    "def sanitize_sheet_name(sheet_name):\n",
    "    # List of illegal characters in Excel sheet names\n",
    "    illegal_characters = ['\\\\', '/', '?', '*', ':', '[', ']', \"'\", '\"', '<', '>', '|']\n",
    "    \n",
    "    # Replace illegal characters with an underscore or an empty string\n",
    "    for char in illegal_characters:\n",
    "        sheet_name = sheet_name.replace(char, '_')\n",
    "    \n",
    "    # Truncate to 31 characters (Excel sheet name limit)\n",
    "    return sheet_name[:31]\n",
    "\n",
    "# Function to sanitize cell values (remove control characters)\n",
    "def sanitize_cell_value(value):\n",
    "    # Remove any non-printable ASCII characters (values 0-31) and control characters\n",
    "    if isinstance(value, str):\n",
    "        # Replace characters that are not printable\n",
    "        return ''.join(ch if 32 <= ord(ch) <= 126 else ' ' for ch in value)\n",
    "    return value\n",
    "\n",
    "# Read the schema (Excel file with column names, start indexes, and column lengths)\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "\n",
    "# Extract column names, starting indexes, and lengths from the schema DataFrame\n",
    "columns = df_schema['Column'].tolist()\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Starting index for each column\n",
    "column_lengths = df_schema['Len'].tolist()  # Length of each column in data\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520  # Set as per your example\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "\n",
    "    # Open the gzipped file for reading in binary mode\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        # Read the file in chunks of 'chunk_size'\n",
    "        while True:\n",
    "            # Read the next chunk of data (520 bytes)\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            \n",
    "            # If the chunk is empty, we've reached the end of the file\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            \n",
    "            # Extract data based on the schema (start indexes and column lengths)\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    # Adjust for 1-based to 0-based index:\n",
    "                    start_idx = start - 1  # Adjust the start index (1-based to 0-based)\n",
    "                    end_idx = start_idx + length  # The length determines how much data to extract\n",
    "                    \n",
    "                    # Extract the value from the chunk\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()  # Extract the value\n",
    "                    row_values.append(sanitize_cell_value(value))  # Sanitize cell value\n",
    "\n",
    "            # Append the extracted row to the result list\n",
    "            all_rows.append(row_values)\n",
    "\n",
    "    # Create a DataFrame from the collected rows\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Directory where the .gz files are located\n",
    "gz_file_directory = 'All_gz_files/'  # Replace with the path to your gz files\n",
    "\n",
    "# Output Excel file path\n",
    "output_file = 'extracted_data_combined.xlsx'\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Loop through all gzipped files in the specified directory\n",
    "    for gz_file in os.listdir(gz_file_directory):\n",
    "        if gz_file.endswith('.gz'):  # Ensure we only process .gz files\n",
    "            gz_file_path = os.path.join(gz_file_directory, gz_file)\n",
    "            \n",
    "            # Process the gzipped file and extract data\n",
    "            df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "            \n",
    "            # Use the filename (without extension) as the sheet name, sanitized and truncated\n",
    "            sheet_name = os.path.splitext(gz_file)[0]\n",
    "            sheet_name = sanitize_sheet_name(sheet_name)  # Sanitize sheet name\n",
    "            \n",
    "            # Write the DataFrame to the Excel file in the corresponding sheet\n",
    "            df_extracted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60de93-d9eb-40e6-9514-3cd57219a20c",
   "metadata": {},
   "source": [
    "<h3>End file logic applied so if _End file will not present in our system for respective file data will not going to load in our destination</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06e9830b-a4d0-4a3b-bea7-51552ca731a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data was extracted from any files.\n",
      "Data has been successfully extracted and saved to extracted_data_combined_withoutEnd.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "# Function to sanitize sheet names (remove illegal characters for Excel)\n",
    "def sanitize_sheet_name(sheet_name):\n",
    "    illegal_characters = ['\\\\', '/', '?', '*', ':', '[', ']', \"'\", '\"', '<', '>', '|']\n",
    "    for char in illegal_characters:\n",
    "        sheet_name = sheet_name.replace(char, '_')\n",
    "    return sheet_name[:31]  # Excel sheet names can only be 31 characters max\n",
    "\n",
    "# Function to sanitize cell values (remove control characters)\n",
    "def sanitize_cell_value(value):\n",
    "    if isinstance(value, str):\n",
    "        return ''.join(ch if 32 <= ord(ch) <= 126 else ' ' for ch in value)\n",
    "    return value\n",
    "\n",
    "# Read the schema (Excel file with column names, start indexes, and column lengths)\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "\n",
    "# Extract column names, starting indexes, and lengths from the schema DataFrame\n",
    "columns = df_schema['Column'].tolist()\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Starting index for each column\n",
    "column_lengths = df_schema['Len'].tolist()  # Length of each column in data\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520  # Set as per your example\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    start_idx = start - 1\n",
    "                    end_idx = start_idx + length\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()\n",
    "                    row_values.append(sanitize_cell_value(value))\n",
    "            all_rows.append(row_values)\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Directory where the .gz files are located\n",
    "gz_file_directory = 'All_gz_files/'  # Replace with the path to your gz files\n",
    "\n",
    "# Output Excel file path\n",
    "output_file = 'extracted_data_combined_withoutEnd.xlsx'\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    sheet_written = False  # Flag to track if at least one sheet is written\n",
    "    for gz_file in os.listdir(gz_file_directory):\n",
    "        if gz_file.endswith('.gz'):\n",
    "            # Check if the \"_END\" file exists for the corresponding main file\n",
    "            main_file = gz_file.split('_END')[0]  # Split main part of the file name\n",
    "            end_file = f\"{main_file}_END.gz\"\n",
    "            \n",
    "            # Check if the corresponding \"_END.gz\" file exists\n",
    "            if os.path.exists(os.path.join(gz_file_directory, end_file)):  # Only process if end file exists\n",
    "                gz_file_path = os.path.join(gz_file_directory, gz_file)\n",
    "                df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "                \n",
    "                # Ensure the sheet name is sanitized to avoid issues with Excel sheet names\n",
    "                sheet_name = sanitize_sheet_name(os.path.splitext(gz_file)[0])\n",
    "                df_extracted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                sheet_written = True  # Mark that at least one sheet has been added\n",
    "    \n",
    "    # If no data files were processed, create an empty default sheet\n",
    "    if not sheet_written:\n",
    "        print(\"No data was extracted from any files.\")\n",
    "        # Create a default empty DataFrame and add it as a sheet\n",
    "        empty_df = pd.DataFrame(columns=columns)\n",
    "        empty_df.to_excel(writer, sheet_name='No Data', index=False)\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf0052-3178-4ec7-bc7f-dc9c1472bcc5",
   "metadata": {},
   "source": [
    "<h3>Main and End File Handling:\n",
    "For each .gz file in the directory, we check if the corresponding \"end\" file (with the _END.gz suffix) exists.\n",
    "We do this by splitting the filename at _END and checking for the presence of the end file. If the end file exists, we proceed to read and process the main file.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5410b716-ad2c-4d9b-9821-3b6d79f512be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c5c824-6660-453f-a23c-fc01dbb87db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sanitize sheet names (remove illegal characters for Excel)\n",
    "def sanitize_sheet_name(sheet_name):\n",
    "    illegal_characters = ['\\\\', '/', '?', '*', ':', '[', ']', \"'\", '\"', '<', '>', '|']\n",
    "    for char in illegal_characters:\n",
    "        sheet_name = sheet_name.replace(char, '_')\n",
    "    return sheet_name[:31]  # Excel sheet names can only be 31 characters max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274fb3dc-e407-4acd-953b-e911d6ba65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sanitize cell values (remove control characters)\n",
    "def sanitize_cell_value(value):\n",
    "    if isinstance(value, str):\n",
    "        return ''.join(ch if 32 <= ord(ch) <= 126 else ' ' for ch in value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e64e67-366f-4e1a-b87b-ab286bef1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8820b83-a3ce-467b-99da-0fbd719cabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names, starting indexes, and lengths from the schema DataFrame\n",
    "columns = df_schema['Column'].tolist()\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Starting index for each column\n",
    "column_lengths = df_schema['Len'].tolist()  # Length of each column in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b228dc-700b-4e03-9889-c6ab8e489146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520  # Set as per your our data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c236a883-8a0c-4091-a25f-bfbb244938fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    start_idx = start - 1\n",
    "                    end_idx = start_idx + length\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()\n",
    "                    row_values.append(sanitize_cell_value(value))\n",
    "            all_rows.append(row_values)\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9de6b8-866e-4661-9bf9-7bf5a72cbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the .gz files are located\n",
    "gz_file_directory = 'All_gz_files/'  # Replace with the path to your gz files\n",
    "\n",
    "# Output Excel file path\n",
    "output_file = 'extracted_data_combined_with_END.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8699c5b-1f85-46eb-83e1-74a894172bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "No data was extracted from any files.\n",
      "Data has been successfully extracted and saved to extracted_data_combined_with_END.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    sheet_written = False  # Flag to track if at least one sheet is written\n",
    "    for gz_file in os.listdir(gz_file_directory):\n",
    "        if gz_file.endswith('.gz'):\n",
    "            # Check if the \"_END\" file exists for the corresponding main file\n",
    "            main_file = gz_file.split('_END')[0]  # Split main part of the file name\n",
    "            end_file = f\"{main_file}_END.gz\"\n",
    "            \n",
    "            # Check if the corresponding \"_END.gz\" file exists\n",
    "            if os.path.exists(os.path.join(gz_file_directory, end_file)):  # Only process if end file exists\n",
    "                gz_file_path = os.path.join(gz_file_directory, gz_file)\n",
    "                df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "                \n",
    "                # Ensure the sheet name is sanitized to avoid issues with Excel sheet names\n",
    "                sheet_name = sanitize_sheet_name(os.path.splitext(gz_file)[0])\n",
    "                df_extracted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                sheet_written = True  # Mark that at least one sheet has been added\n",
    "           # If no data files were processed, create an empty default sheet\n",
    "            if not sheet_written:\n",
    "                print(\"No data was extracted from any files.\")\n",
    "                # Create a default empty DataFrame and add it as a sheet\n",
    "                empty_df = pd.DataFrame(columns=columns)\n",
    "                empty_df.to_excel(writer, sheet_name='No Data', index=False)\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03f79c6-f653-44a4-a6b8-ec205db6eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in schema file: ['Column', 'Starting Index', 'Len']\n",
      "Data has been successfully extracted and saved to combined_dta.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sanitize sheet names (remove illegal characters for Excel)\n",
    "def sanitize_sheet_name(sheet_name):\n",
    "    illegal_characters = ['\\\\', '/', '?', '*', ':', '[', ']', \"'\", '\"', '<', '>', '|']\n",
    "    for char in illegal_characters:\n",
    "        sheet_name = sheet_name.replace(char, '_')\n",
    "    return sheet_name[:31]  # Excel sheet names can only be 31 characters max\n",
    "\n",
    "# Function to sanitize cell values (remove control characters)\n",
    "def sanitize_cell_value(value):\n",
    "    if isinstance(value, str):\n",
    "        return ''.join(ch if 32 <= ord(ch) <= 126 else ' ' for ch in value)\n",
    "    return value\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    start_idx = start - 1\n",
    "                    end_idx = start_idx + length\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()\n",
    "                    row_values.append(sanitize_cell_value(value))\n",
    "            all_rows.append(row_values)\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Directory where the .gz files are located\n",
    "gz_file_directory = 'All_gz_files/'  # Replace with the path to your gz files\n",
    "\n",
    "# Output Excel file path\n",
    "output_file = 'combined_dta.xlsx'\n",
    "# Read schema file\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "\n",
    "# Debugging: Display the actual column names\n",
    "print(\"Available columns in schema file:\", df_schema.columns.tolist())\n",
    "\n",
    "# Map actual schema columns to the expected variables\n",
    "columns = df_schema['Column'].tolist()  # Use the 'Column' column for column names\n",
    "start_indexes = df_schema['Starting Index'].tolist()  # Use the 'Starting Index' column for start indexes\n",
    "column_lengths = df_schema['Len'].tolist()  # Use the 'Len' column for column lengths\n",
    "\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    sheet_written = False  # Flag to track if at least one sheet is written\n",
    "    \n",
    "    for gz_file in os.listdir(gz_file_directory):\n",
    "        if gz_file.endswith('.gz') and not gz_file.endswith('_END.gz'):\n",
    "            # Original file path\n",
    "            gz_file_path = os.path.join(gz_file_directory, gz_file)\n",
    "\n",
    "            # Check for corresponding \"_END\" file\n",
    "            main_file = gz_file.split('_')[0]  # Extract the main file identifier\n",
    "            end_file = f\"{main_file}_END\"\n",
    "            end_file_path = os.path.join(gz_file_directory, end_file)\n",
    "            \n",
    "            # Logic to decide whether to process the original file\n",
    "            process_file = False\n",
    "            \n",
    "            if os.path.exists(end_file_path):\n",
    "                # Check if the \"_END\" file is empty\n",
    "                if os.path.getsize(end_file_path) == 0:\n",
    "                    process_file = True\n",
    "            else:\n",
    "                # If \"_END\" file does not exist, process the original file\n",
    "                process_file = True\n",
    "            \n",
    "            if process_file:\n",
    "                # Process the file\n",
    "                df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "                \n",
    "                # Ensure the sheet name is sanitized\n",
    "                sheet_name = sanitize_sheet_name(os.path.splitext(gz_file)[0])\n",
    "                df_extracted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                sheet_written = True  # Mark that at least one sheet has been added\n",
    "    \n",
    "    # If no data files were processed, create an empty default sheet\n",
    "    if not sheet_written:\n",
    "        print(\"No data was extracted from any files.\")\n",
    "        # Create a default empty DataFrame and add it as a sheet\n",
    "        empty_df = pd.DataFrame(columns=columns)\n",
    "        empty_df.to_excel(writer, sheet_name='No Data', index=False)\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c07947d-a5b6-4d95-bc6f-3694c2c0f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping R520000220240201120001_2024020112050021076.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000220240201170004_2024020117050024632.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201110000_202402011105005819.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201120008_2024020112050021076.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201130008_202402011305002964.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201140006_2024020114050016558.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201150003_2024020115050030666.gz because corresponding '_END' file is missing.\n",
      "Skipping R520000320240201160002_2024020116050012283.gz because corresponding '_END' file is missing.\n",
      "Data has been successfully extracted and saved to combined_dta.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sanitize sheet names (remove illegal characters for Excel)\n",
    "def sanitize_sheet_name(sheet_name):\n",
    "    illegal_characters = ['\\\\', '/', '?', '*', ':', '[', ']', \"'\", '\"', '<', '>', '|']\n",
    "    for char in illegal_characters:\n",
    "        sheet_name = sheet_name.replace(char, '_')\n",
    "    return sheet_name[:31]  # Excel sheet names can only be 31 characters max\n",
    "\n",
    "# Function to sanitize cell values (remove control characters)\n",
    "def sanitize_cell_value(value):\n",
    "    if isinstance(value, str):\n",
    "        return ''.join(ch if 32 <= ord(ch) <= 126 else ' ' for ch in value)\n",
    "    return value\n",
    "\n",
    "# Function to check if an \"_END\" file is empty\n",
    "def is_end_file_empty(end_file_path):\n",
    "    try:\n",
    "        with open(end_file_path, 'r') as f:\n",
    "            return f.read().strip() == ''\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "# Function to process gzipped file and extract data based on schema\n",
    "def process_gzipped_file(file_path, chunk_size, start_indexes, column_lengths, columns):\n",
    "    all_rows = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            chunk_data = f.read(chunk_size)\n",
    "            if not chunk_data:\n",
    "                break\n",
    "            row_values = []\n",
    "            for i, start in enumerate(start_indexes):\n",
    "                length = column_lengths[i]\n",
    "                if length is not None:\n",
    "                    start_idx = start - 1\n",
    "                    end_idx = start_idx + length\n",
    "                    value = chunk_data[start_idx:end_idx].decode('utf-8', errors='ignore').strip()\n",
    "                    row_values.append(sanitize_cell_value(value))\n",
    "            all_rows.append(row_values)\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Directory where the .gz files are located\n",
    "gz_file_directory = 'All_gz_files/'  # Replace with the path to your gz files\n",
    "\n",
    "# Output Excel file path\n",
    "output_file = 'combined_dta.xlsx'\n",
    "\n",
    "# Load schema\n",
    "schema_file = 'extracted_data.xlsx'  # Path to your schema file\n",
    "df_schema = pd.read_excel(schema_file)\n",
    "start_indexes = df_schema['Starting Index'].tolist()\n",
    "column_lengths = df_schema['Len'].tolist()\n",
    "columns = df_schema['Column'].tolist()\n",
    "\n",
    "# Define the chunk size (520 bytes per row)\n",
    "chunk_size = 520\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    sheet_written = False  # Flag to track if at least one sheet is written\n",
    "\n",
    "    for gz_file in os.listdir(gz_file_directory):\n",
    "        if gz_file.endswith('.gz'):\n",
    "            # Get the base name of the gzipped file\n",
    "            base_name = gz_file.split('_2024')[0]\n",
    "\n",
    "            # Check for the \"_END\" file\n",
    "            end_file = f\"{base_name}_END\"  # \"_END\" file without extension\n",
    "            end_file_path = os.path.join(gz_file_directory, end_file)\n",
    "\n",
    "            # Process only if the \"_END\" file exists and is empty\n",
    "            if os.path.exists(end_file_path):\n",
    "                if is_end_file_empty(end_file_path):\n",
    "                    # Process the gzipped file\n",
    "                    gz_file_path = os.path.join(gz_file_directory, gz_file)\n",
    "                    df_extracted = process_gzipped_file(gz_file_path, chunk_size, start_indexes, column_lengths, columns)\n",
    "\n",
    "                    # Ensure the sheet name is sanitized to avoid issues with Excel sheet names\n",
    "                    sheet_name = sanitize_sheet_name(os.path.splitext(gz_file)[0])\n",
    "                    df_extracted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    sheet_written = True  # Mark that at least one sheet has been added\n",
    "                else:\n",
    "                    print(f\"Skipping {gz_file} because corresponding '_END' file is not empty.\")\n",
    "            else:\n",
    "                print(f\"Skipping {gz_file} because corresponding '_END' file is missing.\")\n",
    "\n",
    "    # If no data files were processed, create an empty default sheet\n",
    "    if not sheet_written:\n",
    "        print(\"No data was extracted from any files.\")\n",
    "        empty_df = pd.DataFrame(columns=columns)\n",
    "        empty_df.to_excel(writer, sheet_name='No Data', index=False)\n",
    "\n",
    "print(f\"Data has been successfully extracted and saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6b513-33f1-4036-a6c5-6d21821f0bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
